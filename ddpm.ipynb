{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0F0PAzBQxvvFdQEJNjK2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dage1210/dage1210.github.io/blob/main/ddpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Y6i87QCIKB3"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional\n",
        "from typing import Union\n",
        "from typing import Tuple\n",
        "from typing import List\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip diffusion-model-main.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8vq0l-TIQdS",
        "outputId": "f304948e-7fc9-4d49-9adb-5603f586f775"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  diffusion-model-main.zip\n",
            "a1b4b7fa59ca01d470dc8e67814d4404f0f835bc\n",
            "   creating: diffusion-model-main/\n",
            "  inflating: diffusion-model-main/README.md  \n",
            "  inflating: diffusion-model-main/main.ipynb  \n",
            "  inflating: diffusion-model-main/mnist_test.csv  \n",
            " extracting: diffusion-model-main/mnist_train.rar  \n",
            "  inflating: diffusion-model-main/u_net.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd diffusion-model-main\n"
      ],
      "metadata": {
        "id": "d6rBN13-JDQ6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip mnist_train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olm53A_1JHfp",
        "outputId": "2dd47ae5-01b1-4b19-99ff-4d7e7515bd6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  mnist_train.zip\n",
            "  inflating: mnist_train.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir= './mnist_train.csv'\n",
        "test_dir= './mnist_test.csv'"
      ],
      "metadata": {
        "id": "oV_V5Iu0J8hP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_mnist(file_name):\n",
        "    mnist= []\n",
        "    # with open(file_name, encoding= 'utf-8') as f:\n",
        "    with open(file_name, 'r', encoding='utf-8-sig') as f:\n",
        "        mnist= f.readlines()\n",
        "    rows= len(mnist)\n",
        "    mnist= np.array([int(item) for sting in mnist for item in sting.split(',')]).reshape(rows, -1)\n",
        "    return torch.from_numpy(mnist[:, 1:]).float()/ 255.0, torch.from_numpy(mnist[:, 0])"
      ],
      "metadata": {
        "id": "c2HY0sc7KGMK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip mnist_train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyXXS0_MLeFh",
        "outputId": "ed373457-5389-4078-f8ea-9ec93b1e6344"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  mnist_train.zip\n",
            "  inflating: mnist_train.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_mnist(file_name):\n",
        "    data = []\n",
        "    with open(file_name, encoding='latin-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            try:\n",
        "                row = list(map(int, parts))\n",
        "                data.append(row)\n",
        "            except ValueError:\n",
        "                continue   # 跳过脏行\n",
        "    data = np.array(data, dtype=np.int32)\n",
        "    return torch.tensor(data[:, 1:], dtype=torch.float32) / 255.0, \\\n",
        "           torch.tensor(data[:, 0], dtype=torch.long)"
      ],
      "metadata": {
        "id": "GRq16bdPOWVc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed, batch_size= 1, 32\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "pics, labels= read_mnist(train_dir)\n",
        "random_num= torch.rand(len(pics))\n",
        "train_flag, valid_flag, test_flag= random_num< torch.tensor(0.8), (random_num>= torch.tensor(0.8))* (random_num< torch.tensor(0.9)), random_num>= torch.tensor(0.9)\n",
        "train_pics, valid_pics, test_pics= pics[train_flag], pics[valid_flag], pics[test_flag]\n",
        "train_labels, valid_labels, test_labels= labels[train_flag], labels[valid_flag], labels[test_flag]\n",
        "train_set, valid_set, test_set= torch.utils.data.TensorDataset(train_pics, train_labels), torch.utils.data.TensorDataset(valid_pics, valid_labels), torch.utils.data.TensorDataset(test_pics, test_labels)\n",
        "train_loader, valid_loader, test_loader= torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle= True), torch.utils.data.DataLoader(valid_set, batch_size= batch_size, shuffle= True), torch.utils.data.DataLoader(test_set, batch_size= batch_size, shuffle= True)"
      ],
      "metadata": {
        "id": "0F_esjBoKP15"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tools:\n",
        "    def gather(self, consts: torch.Tensor, t: torch.Tensor):\n",
        "        c= consts.gather(-1, t)\n",
        "        return c.reshape(-1, 1, 1, 1)"
      ],
      "metadata": {
        "id": "3FC_Oua4Oy_c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenoiseDiffusion():\n",
        "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.eps_model= eps_model\n",
        "        self.beta= torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
        "        self.alpha= 1.0- self.beta\n",
        "        # compute cumulative product\n",
        "        self.alpha_bar= torch.cumprod(self.alpha, dim= 0)\n",
        "        self.n_steps= n_steps\n",
        "        self.sigma= self.beta\n",
        "        self.tools= Tools()\n",
        "\n",
        "    # forward- diffusion\n",
        "    def q_xt_x0(self, x0: torch.tensor, t:torch.Tensor):\n",
        "        # compute mean and var of xt according to x0\n",
        "        # xt= sqrt(at)*x0+ sqrt(1-at)*eps\n",
        "        maen= self.tools.gather(self.alpha_bar, t)** 0.5* x0\n",
        "        # (batch_size, 1, 1, 1)\n",
        "        var= 1- self.tools.gather(self.alpha_bar, t)\n",
        "        return maen, var\n",
        "\n",
        "    # forward- diffusion\n",
        "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor]= None):\n",
        "        # compute xt according mean and var of xt\n",
        "        if eps is None:\n",
        "            eps= torch.randn_like(x0)\n",
        "        maen, var= self.q_xt_x0(x0, t)\n",
        "        return maen+ (var** 0.5)* eps\n",
        "\n",
        "    # sampling\n",
        "    def p_sample(self, xt: torch.tensor, t: torch.Tensor):\n",
        "        # compute xt-1 according xt\n",
        "        eps_hat= self.eps_model(xt, t)\n",
        "        alpha_bar= self.tools.gather(self.alpha_bar, t)\n",
        "        alpha= self.tools.gather(self.alpha, t)\n",
        "        eps_coef= (1- alpha)/ (1- alpha_bar)** 0.5\n",
        "        maen= 1/ (alpha** 0.5)* (xt- eps_coef* eps_hat)\n",
        "        var= self.tools.gather(self.sigma, t)\n",
        "        eps= torch.randn(xt.shape, device= xt.device)\n",
        "        return maen+ (var** 0.5)* eps\n",
        "\n",
        "    # loss\n",
        "    # x0, (batch_size, C, H, W);\n",
        "    def loss(self, x0: torch.tensor, noise: Optional[torch.Tensor]= None):\n",
        "        # distance between loss\n",
        "        batch_size= x0.shape[0]\n",
        "        # (batch_size, )\n",
        "        t= torch.randint(0, self.n_steps, (batch_size, ), device= x0.device, dtype= torch.long)\n",
        "        if noise is None:noise= torch.randn_like(x0)\n",
        "        xt= self.q_sample(x0, t, eps= noise)\n",
        "        eps_hat= self.eps_model(xt, t)\n",
        "        return F.mse_loss(noise, eps_hat)"
      ],
      "metadata": {
        "id": "6_b_i4w-O0LW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x* torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "lCo7ic3iO3fR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBolck(nn.Module):\n",
        "    # n_groups, hyper-parameter of group norm\n",
        "    # group norm, group normalize; first, split channels into different groups; then, normalize feature in every group, as batch_normalization, it has some hyper-parameters.\n",
        "    # feat_map\n",
        "    # cv1(feat_map)+ cv2(time_emb) -> feat_map' + cv(feat_map) -> output\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int= 16, dropout: float= 0.1):\n",
        "        super().__init__()\n",
        "        # in_channels// n_groups\n",
        "        self.norm1= nn.GroupNorm(n_groups, in_channels)\n",
        "        self.act1= Swish()\n",
        "        self.conv1= nn.Conv2d(in_channels, out_channels, kernel_size= (3, 3), padding= (1, 1))\n",
        "        self.norm2= nn.GroupNorm(n_groups, out_channels)\n",
        "        self.act2= Swish()\n",
        "        self.conv2= nn.Conv2d(out_channels, out_channels, kernel_size= (3, 3), padding= (1, 1))\n",
        "        if in_channels!= out_channels:\n",
        "            self.shortcut= nn.Conv2d(in_channels, out_channels, kernel_size= (1, 1))\n",
        "        else:\n",
        "            self.shortcut= nn.Identity()\n",
        "        self.time_emb= nn.Linear(time_channels, out_channels)\n",
        "        self.time_act= Swish()\n",
        "        self.dropout= nn.Dropout(dropout)\n",
        "    def forward(self, x: torch.Tensor, t:torch.Tensor):\n",
        "        # norm>> act>> conv\n",
        "        h= self.conv1(self.act1(self.norm1(x)))\n",
        "        # time embedding, (batch_size, out_channels, 1, 1)\n",
        "        # time embedding的不同特征与不同通道的特征图进行相加以实现空间与时间的融合\n",
        "        h+= self.time_emb(self.time_act(t))[:, :, None, None]\n",
        "        h= self.conv2(self.dropout(self.act2(self.norm2(h))))\n",
        "        return h+ self.shortcut(x)"
      ],
      "metadata": {
        "id": "n97PIqZ3O5yH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, n_channels: int, n_heads: int= 1, k_dims: int= None, n_groups: int= 16):\n",
        "        super().__init__()\n",
        "        if k_dims is None:\n",
        "            k_dims= n_channels\n",
        "        self.norm= nn.GroupNorm(n_groups, n_channels)\n",
        "        # as n_channels= 64, 64>> 8* 128* 3\n",
        "        self.projection= nn.Linear(n_channels, n_heads* k_dims* 3)\n",
        "        self.output= nn.Linear(n_heads* k_dims, n_channels)\n",
        "        self.scale= k_dims** -0.5\n",
        "        self.n_heads= n_heads\n",
        "        self.k_dims= k_dims\n",
        "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor]= None):\n",
        "        _= t\n",
        "        batch_size, n_channels, height, weight= x.shape\n",
        "        # pull x straight, (batch_size, n_channels, H* W)>> (batch_size, H* W, n_channels)\n",
        "        x= x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
        "        # (batch_size, H* W, channels)>> (batch_size, H* W, head, 3* k_head_dim)\n",
        "        qkv= self.projection(x).view(batch_size, -1, self.n_heads, 3* self.k_dims)\n",
        "        # q, (batch_size, H* W, head, k_head_dim); k, (batch_size, H* W, head, k_head_dim); v, (,,).\n",
        "        q, k, v= torch.chunk(qkv, 3, dim= -1)\n",
        "        # (batch_size, H* W, head, dim), (batch_size, H* W, head, dim) -> (batch_size, H* W, H* W, head)\n",
        "        # This writing style is really good!\n",
        "        attn= torch.einsum('bihd,bjhd->bijh', q, k)* self.scale\n",
        "        attn= attn.softmax(dim= 2)\n",
        "        # (batch_size, H* W, H* W, head), (batch_size, H* W, head, dim)\n",
        "        res= torch.einsum('bijh,bjhd->bihd', attn, v)\n",
        "        # (batch_size, H* W, head* dim)\n",
        "        res= res.view(batch_size, -1, self.n_heads* self.k_dims)\n",
        "        # (batch_size, H* W, C)\n",
        "        res= self.output(res)\n",
        "        res+= x\n",
        "        res= res.permute(0, 2, 1).view(batch_size, n_channels, height, weight)\n",
        "        return res"
      ],
      "metadata": {
        "id": "nndRz-rFO8y3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "    # Encoder\n",
        "    # DownBlock= ResidualBlock+ AttentionBlock\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        self.res= ResidualBolck(in_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn= AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn= nn.Identity()\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x= self.res(x, t)\n",
        "        x= self.attn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "D8xRD9AUPAgN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, n_channels:int):\n",
        "        super().__init__()\n",
        "        self.n_channels= n_channels\n",
        "        # n_channels// 8 and concate\n",
        "        self.lin1= nn.Linear(self.n_channels// 4, self.n_channels)\n",
        "        self.act= Swish()\n",
        "        self.lin2= nn.Linear(self.n_channels, self.n_channels)\n",
        "    def forward(self, t: torch.Tensor):\n",
        "        half_dim= self.n_channels// 8\n",
        "        emb= math.log(10000)/ (half_dim- 1)\n",
        "        emb= torch.exp(torch.arange(half_dim, device= t.device)* -emb)\n",
        "        emb= t[:, None]* emb[None, :]\n",
        "        emb= torch.cat((emb.sin(), emb.cos()), dim= 1)\n",
        "        # transform\n",
        "        emb= self.act(self.lin1(emb))\n",
        "        emb= self.lin2(emb)\n",
        "        return emb"
      ],
      "metadata": {
        "id": "1PAAFJGMPDUv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(nn.Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        # 反卷积，\n",
        "        self.conv= nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        _= t\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "c7UgmGIAPGTW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv= nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
        "    def forward(self, x:torch.tensor, t:torch.tensor):\n",
        "        _= t\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "PSTcMQyFPI1X"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiddleBlock(nn.Module):\n",
        "    def __init__(self, n_channels:int, time_channels:int):\n",
        "        super().__init__()\n",
        "        self.res1= ResidualBolck(n_channels, n_channels, time_channels)\n",
        "        self.attn= AttentionBlock(n_channels)\n",
        "        self.res2= ResidualBolck(n_channels, n_channels, time_channels)\n",
        "    def forward(self, x:torch.tensor, t:torch.tensor):\n",
        "        x= self.res1(x, t)\n",
        "        x= self.attn(x)\n",
        "        x= self.res2(x, t)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XDcZ8j4jPLB3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        # we concatenate the output of the same resolution\n",
        "        self.res= ResidualBolck(in_channels+ out_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn= AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn= nn.Identity()\n",
        "    def forward(self, x:torch.Tensor, t:torch.Tensor):\n",
        "        x= self.res(x, t)\n",
        "        x= self.attn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L6aMe1NxPNmH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, image_channels: int= 1, n_channels: int= 16, ch_mults: Union[tuple[int, ...], List[int]]= [1, 2, 2, 2], is_attn: Union[Tuple[bool, ...], List[int]]= (False, False, False, False), n_blocks: int= 1):\n",
        "        super().__init__()\n",
        "        #\n",
        "        n_resolutions= len(ch_mults)\n",
        "        self.image_proj= nn.Conv2d(image_channels, n_channels, kernel_size= (3, 3), padding= (1, 1))\n",
        "        self.time_emb= TimeEmbedding(n_channels* 4)\n",
        "        # ---- Encoder ----\n",
        "        down= []\n",
        "        out_channels= in_channels= n_channels\n",
        "        # for each layers\n",
        "        for i in range(n_resolutions):\n",
        "            # 16* (1, 2, 2, 2)\n",
        "            out_channels= in_channels* ch_mults[i]\n",
        "            for _ in range(n_blocks):\n",
        "                # residual+ attn\n",
        "                down.append(DownBlock(in_channels, out_channels, n_channels* 4, is_attn[i]))\n",
        "                in_channels= out_channels\n",
        "            # downsample in each layer\n",
        "            if i< n_resolutions- 1:\n",
        "                # channels no change, (H, W)>> (H/2, W/ 2)\n",
        "                down.append(Downsample(in_channels))\n",
        "        self.down= nn.ModuleList(down)\n",
        "        # middle, residual+ attn+ residual\n",
        "        self.middle= MiddleBlock(out_channels, n_channels* 4)\n",
        "        # Decoder\n",
        "        up= []\n",
        "        in_channels= out_channels\n",
        "        # for each layer\n",
        "        for i in reversed(range(n_resolutions)):\n",
        "            #\n",
        "            out_channels= in_channels\n",
        "            for _ in range(n_blocks):\n",
        "                # residual+ attn\n",
        "                up.append(UpBlock(in_channels, out_channels, n_channels* 4, is_attn[i]))\n",
        "            out_channels= in_channels// ch_mults[i]\n",
        "            up.append(UpBlock(in_channels, out_channels, n_channels* 4, is_attn[i]))\n",
        "            in_channels= out_channels\n",
        "            if i> 0:\n",
        "                up.append(Upsample(in_channels))\n",
        "        self.up= nn.ModuleList(up)\n",
        "        self.norm= nn.GroupNorm(8, n_channels)\n",
        "        self.act= Swish()\n",
        "        self.final= nn.Conv2d(in_channels, image_channels, kernel_size= (3, 3), padding= (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        t= self.time_emb(t)\n",
        "        x= self.image_proj(x)\n",
        "        # Encoder\n",
        "        h= [x]\n",
        "        for m in self.down:\n",
        "            x= m(x, t)\n",
        "            h.append(x)\n",
        "        # Middle\n",
        "        x= self.middle(x, t)\n",
        "        # Decoder\n",
        "        for m in self.up:\n",
        "            if isinstance(m, Upsample):\n",
        "                x= m(x, t)\n",
        "            else:\n",
        "                s= h.pop()\n",
        "                x= torch.cat((x, s), dim= 1)\n",
        "                x= m(x, t)\n",
        "        return self.final(self.act(self.norm(x)))"
      ],
      "metadata": {
        "id": "KasOzcokPQ1e"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr, device, save_dir= 0.001, 'cuda:0', './u_net.pt'\n",
        "lr, device, save_dir= 0.001, 'cpu', './u_net-cpu.pt'\n",
        "u_net= UNet(1, 16, [1, 2, 2], [False, False, False], n_blocks= 1).to(device)\n",
        "dm= DenoiseDiffusion(u_net, 1000, device= device)\n",
        "opt_dm= torch.optim.Adam(u_net.parameters(), lr= lr)"
      ],
      "metadata": {
        "id": "4fYiE3F6PTw-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score, score, epochs, early_stop_time, early_stop_threshold= 1e10, 0, 200, 0, 40\n",
        "for epoch in range(epochs):\n",
        "    loss_record= []\n",
        "    for step, (pic, labels) in enumerate(train_loader):\n",
        "        if step%20==0:\n",
        "          print('{}:{}'.format(step,epoch))\n",
        "        pic= pic.view(-1, 1, 28, 28).to(device)\n",
        "        opt_dm.zero_grad()\n",
        "        loss= dm.loss(pic)\n",
        "        loss_record.append(loss.item())\n",
        "        loss.backward()\n",
        "        opt_dm.step()\n",
        "    print(f'training epoch: {epoch}, mean loss: {torch.tensor(loss_record).mean()}')\n",
        "    loss_record= []\n",
        "    with torch.no_grad():\n",
        "        for step, (pic, labels) in enumerate(valid_loader):\n",
        "            pic= pic.view(-1, 1, 28, 28).to(device)\n",
        "            loss= dm.loss(pic)\n",
        "            loss_record.append(loss.item())\n",
        "    mean_loss= torch.tensor(loss_record).mean()\n",
        "    # early stopping\n",
        "    if mean_loss< best_score:\n",
        "        early_stop_time= 0\n",
        "        best_score= mean_loss\n",
        "        torch.save(u_net, f'{save_dir}')\n",
        "    else:\n",
        "        early_stop_time= early_stop_time+ 1\n",
        "    if early_stop_time> early_stop_threshold:\n",
        "        break\n",
        "    # output\n",
        "    print(f'early_stop_time/early_stop_threshold: {early_stop_time}/{early_stop_threshold}, mean loss: {mean_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCNh2qcVPWtv",
        "outputId": "5e1140ad-fcb0-442e-fb2c-8004292e2f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:0\n",
            "20:0\n",
            "40:0\n",
            "60:0\n",
            "80:0\n",
            "100:0\n",
            "120:0\n",
            "140:0\n",
            "160:0\n",
            "180:0\n",
            "200:0\n",
            "220:0\n",
            "240:0\n",
            "260:0\n",
            "280:0\n",
            "300:0\n",
            "320:0\n",
            "340:0\n",
            "360:0\n",
            "380:0\n",
            "400:0\n",
            "420:0\n",
            "440:0\n",
            "460:0\n",
            "480:0\n",
            "500:0\n",
            "520:0\n",
            "540:0\n",
            "560:0\n",
            "580:0\n",
            "600:0\n",
            "620:0\n",
            "640:0\n",
            "660:0\n",
            "680:0\n",
            "700:0\n",
            "720:0\n",
            "740:0\n",
            "760:0\n",
            "780:0\n",
            "800:0\n",
            "820:0\n",
            "840:0\n",
            "860:0\n",
            "880:0\n",
            "900:0\n",
            "920:0\n",
            "940:0\n",
            "960:0\n",
            "980:0\n",
            "1000:0\n",
            "1020:0\n",
            "1040:0\n",
            "1060:0\n",
            "1080:0\n",
            "1100:0\n",
            "1120:0\n",
            "1140:0\n",
            "1160:0\n",
            "1180:0\n",
            "1200:0\n",
            "1220:0\n",
            "1240:0\n",
            "1260:0\n",
            "1280:0\n",
            "1300:0\n",
            "1320:0\n",
            "1340:0\n",
            "1360:0\n",
            "1380:0\n",
            "1400:0\n",
            "1420:0\n",
            "1440:0\n",
            "1460:0\n",
            "1480:0\n",
            "training epoch: 0, mean loss: 0.03267353028059006\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.022359013557434082\n",
            "0:1\n",
            "20:1\n",
            "40:1\n",
            "60:1\n",
            "80:1\n",
            "100:1\n",
            "120:1\n",
            "140:1\n",
            "160:1\n",
            "180:1\n",
            "200:1\n",
            "220:1\n",
            "240:1\n",
            "260:1\n",
            "280:1\n",
            "300:1\n",
            "320:1\n",
            "340:1\n",
            "360:1\n",
            "380:1\n",
            "400:1\n",
            "420:1\n",
            "440:1\n",
            "460:1\n",
            "480:1\n",
            "500:1\n",
            "520:1\n",
            "540:1\n",
            "560:1\n",
            "580:1\n",
            "600:1\n",
            "620:1\n",
            "640:1\n",
            "660:1\n",
            "680:1\n",
            "700:1\n",
            "720:1\n",
            "740:1\n",
            "760:1\n",
            "780:1\n",
            "800:1\n",
            "820:1\n",
            "840:1\n",
            "860:1\n",
            "880:1\n",
            "900:1\n",
            "920:1\n",
            "940:1\n",
            "960:1\n",
            "980:1\n",
            "1000:1\n",
            "1020:1\n",
            "1040:1\n",
            "1060:1\n",
            "1080:1\n",
            "1100:1\n",
            "1120:1\n",
            "1140:1\n",
            "1160:1\n",
            "1180:1\n",
            "1200:1\n",
            "1220:1\n",
            "1240:1\n",
            "1260:1\n",
            "1280:1\n",
            "1300:1\n",
            "1320:1\n",
            "1340:1\n",
            "1360:1\n",
            "1380:1\n",
            "1400:1\n",
            "1420:1\n",
            "1440:1\n",
            "1460:1\n",
            "1480:1\n",
            "training epoch: 1, mean loss: 0.021081339567899704\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.019394226372241974\n",
            "0:2\n",
            "20:2\n",
            "40:2\n",
            "60:2\n",
            "80:2\n",
            "100:2\n",
            "120:2\n",
            "140:2\n",
            "160:2\n",
            "180:2\n",
            "200:2\n",
            "220:2\n",
            "240:2\n",
            "260:2\n",
            "280:2\n",
            "300:2\n",
            "320:2\n",
            "340:2\n",
            "360:2\n",
            "380:2\n",
            "400:2\n",
            "420:2\n",
            "440:2\n",
            "460:2\n",
            "480:2\n",
            "500:2\n",
            "520:2\n",
            "540:2\n",
            "560:2\n",
            "580:2\n",
            "600:2\n",
            "620:2\n",
            "640:2\n",
            "660:2\n",
            "680:2\n",
            "700:2\n",
            "720:2\n",
            "740:2\n",
            "760:2\n",
            "780:2\n",
            "800:2\n",
            "820:2\n",
            "840:2\n",
            "860:2\n",
            "880:2\n",
            "900:2\n",
            "920:2\n",
            "940:2\n",
            "960:2\n",
            "980:2\n",
            "1000:2\n",
            "1020:2\n",
            "1040:2\n",
            "1060:2\n",
            "1080:2\n",
            "1100:2\n",
            "1120:2\n",
            "1140:2\n",
            "1160:2\n",
            "1180:2\n",
            "1200:2\n",
            "1220:2\n",
            "1240:2\n",
            "1260:2\n",
            "1280:2\n",
            "1300:2\n",
            "1320:2\n",
            "1340:2\n",
            "1360:2\n",
            "1380:2\n",
            "1400:2\n",
            "1420:2\n",
            "1440:2\n",
            "1460:2\n",
            "1480:2\n",
            "training epoch: 2, mean loss: 0.019171573221683502\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.01806792803108692\n",
            "0:3\n",
            "20:3\n",
            "40:3\n",
            "60:3\n",
            "80:3\n",
            "100:3\n",
            "120:3\n",
            "140:3\n",
            "160:3\n",
            "180:3\n",
            "200:3\n",
            "220:3\n",
            "240:3\n",
            "260:3\n",
            "280:3\n",
            "300:3\n",
            "320:3\n",
            "340:3\n",
            "360:3\n",
            "380:3\n",
            "400:3\n",
            "420:3\n",
            "440:3\n",
            "460:3\n",
            "480:3\n",
            "500:3\n",
            "520:3\n",
            "540:3\n",
            "560:3\n",
            "580:3\n",
            "600:3\n",
            "620:3\n",
            "640:3\n",
            "660:3\n",
            "680:3\n",
            "700:3\n",
            "720:3\n",
            "740:3\n",
            "760:3\n",
            "780:3\n",
            "800:3\n",
            "820:3\n",
            "840:3\n",
            "860:3\n",
            "880:3\n",
            "900:3\n",
            "920:3\n",
            "940:3\n",
            "960:3\n",
            "980:3\n",
            "1000:3\n",
            "1020:3\n",
            "1040:3\n",
            "1060:3\n",
            "1080:3\n",
            "1100:3\n",
            "1120:3\n",
            "1140:3\n",
            "1160:3\n",
            "1180:3\n",
            "1200:3\n",
            "1220:3\n",
            "1240:3\n",
            "1260:3\n",
            "1280:3\n",
            "1300:3\n",
            "1320:3\n",
            "1340:3\n",
            "1360:3\n",
            "1380:3\n",
            "1400:3\n",
            "1420:3\n",
            "1440:3\n",
            "1460:3\n",
            "1480:3\n",
            "training epoch: 3, mean loss: 0.018426761031150818\n",
            "early_stop_time/early_stop_threshold: 1/40, mean loss: 0.01848319172859192\n",
            "0:4\n",
            "20:4\n",
            "40:4\n",
            "60:4\n",
            "80:4\n",
            "100:4\n",
            "120:4\n",
            "140:4\n",
            "160:4\n",
            "180:4\n",
            "200:4\n",
            "220:4\n",
            "240:4\n",
            "260:4\n",
            "280:4\n",
            "300:4\n",
            "320:4\n",
            "340:4\n",
            "360:4\n",
            "380:4\n",
            "400:4\n",
            "420:4\n",
            "440:4\n",
            "460:4\n",
            "480:4\n",
            "500:4\n",
            "520:4\n",
            "540:4\n",
            "560:4\n",
            "580:4\n",
            "600:4\n",
            "620:4\n",
            "640:4\n",
            "660:4\n",
            "680:4\n",
            "700:4\n",
            "720:4\n",
            "740:4\n",
            "760:4\n",
            "780:4\n",
            "800:4\n",
            "820:4\n",
            "840:4\n",
            "860:4\n",
            "880:4\n",
            "900:4\n",
            "920:4\n",
            "940:4\n",
            "960:4\n",
            "980:4\n",
            "1000:4\n",
            "1020:4\n",
            "1040:4\n",
            "1060:4\n",
            "1080:4\n",
            "1100:4\n",
            "1120:4\n",
            "1140:4\n",
            "1160:4\n",
            "1180:4\n",
            "1200:4\n",
            "1220:4\n",
            "1240:4\n",
            "1260:4\n",
            "1280:4\n",
            "1300:4\n",
            "1320:4\n",
            "1340:4\n",
            "1360:4\n",
            "1380:4\n",
            "1400:4\n",
            "1420:4\n",
            "1440:4\n",
            "1460:4\n",
            "1480:4\n",
            "training epoch: 4, mean loss: 0.017811832949519157\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.01756669022142887\n",
            "0:5\n",
            "20:5\n",
            "40:5\n",
            "60:5\n",
            "80:5\n",
            "100:5\n",
            "120:5\n",
            "140:5\n",
            "160:5\n",
            "180:5\n",
            "200:5\n",
            "220:5\n",
            "240:5\n",
            "260:5\n",
            "280:5\n",
            "300:5\n",
            "320:5\n",
            "340:5\n",
            "360:5\n",
            "380:5\n",
            "400:5\n",
            "420:5\n",
            "440:5\n",
            "460:5\n",
            "480:5\n",
            "500:5\n",
            "520:5\n",
            "540:5\n",
            "560:5\n",
            "580:5\n",
            "600:5\n",
            "620:5\n",
            "640:5\n",
            "660:5\n",
            "680:5\n",
            "700:5\n",
            "720:5\n",
            "740:5\n",
            "760:5\n",
            "780:5\n",
            "800:5\n",
            "820:5\n",
            "840:5\n",
            "860:5\n",
            "880:5\n",
            "900:5\n",
            "920:5\n",
            "940:5\n",
            "960:5\n",
            "980:5\n",
            "1000:5\n",
            "1020:5\n",
            "1040:5\n",
            "1060:5\n",
            "1080:5\n",
            "1100:5\n",
            "1120:5\n",
            "1140:5\n",
            "1160:5\n",
            "1180:5\n",
            "1200:5\n",
            "1220:5\n",
            "1240:5\n",
            "1260:5\n",
            "1280:5\n",
            "1300:5\n",
            "1320:5\n",
            "1340:5\n",
            "1360:5\n",
            "1380:5\n",
            "1400:5\n",
            "1420:5\n",
            "1440:5\n",
            "1460:5\n",
            "1480:5\n",
            "training epoch: 5, mean loss: 0.017527135089039803\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.01690533757209778\n",
            "0:6\n",
            "20:6\n",
            "40:6\n",
            "60:6\n",
            "80:6\n",
            "100:6\n",
            "120:6\n",
            "140:6\n",
            "160:6\n",
            "180:6\n",
            "200:6\n",
            "220:6\n",
            "240:6\n",
            "260:6\n",
            "280:6\n",
            "300:6\n",
            "320:6\n",
            "340:6\n",
            "360:6\n",
            "380:6\n",
            "400:6\n",
            "420:6\n",
            "440:6\n",
            "460:6\n",
            "480:6\n",
            "500:6\n",
            "520:6\n",
            "540:6\n",
            "560:6\n",
            "580:6\n",
            "600:6\n",
            "620:6\n",
            "640:6\n",
            "660:6\n",
            "680:6\n",
            "700:6\n",
            "720:6\n",
            "740:6\n",
            "760:6\n",
            "780:6\n",
            "800:6\n",
            "820:6\n",
            "840:6\n",
            "860:6\n",
            "880:6\n",
            "900:6\n",
            "920:6\n",
            "940:6\n",
            "960:6\n",
            "980:6\n",
            "1000:6\n",
            "1020:6\n",
            "1040:6\n",
            "1060:6\n",
            "1080:6\n",
            "1100:6\n",
            "1120:6\n",
            "1140:6\n",
            "1160:6\n",
            "1180:6\n",
            "1200:6\n",
            "1220:6\n",
            "1240:6\n",
            "1260:6\n",
            "1280:6\n",
            "1300:6\n",
            "1320:6\n",
            "1340:6\n",
            "1360:6\n",
            "1380:6\n",
            "1400:6\n",
            "1420:6\n",
            "1440:6\n",
            "1460:6\n",
            "1480:6\n",
            "training epoch: 6, mean loss: 0.017426706850528717\n",
            "early_stop_time/early_stop_threshold: 1/40, mean loss: 0.017131047323346138\n",
            "0:7\n",
            "20:7\n",
            "40:7\n",
            "60:7\n",
            "80:7\n",
            "100:7\n",
            "120:7\n",
            "140:7\n",
            "160:7\n",
            "180:7\n",
            "200:7\n",
            "220:7\n",
            "240:7\n",
            "260:7\n",
            "280:7\n",
            "300:7\n",
            "320:7\n",
            "340:7\n",
            "360:7\n",
            "380:7\n",
            "400:7\n",
            "420:7\n",
            "440:7\n",
            "460:7\n",
            "480:7\n",
            "500:7\n",
            "520:7\n",
            "540:7\n",
            "560:7\n",
            "580:7\n",
            "600:7\n",
            "620:7\n",
            "640:7\n",
            "660:7\n",
            "680:7\n",
            "700:7\n",
            "720:7\n",
            "740:7\n",
            "760:7\n",
            "780:7\n",
            "800:7\n",
            "820:7\n",
            "840:7\n",
            "860:7\n",
            "880:7\n",
            "900:7\n",
            "920:7\n",
            "940:7\n",
            "960:7\n",
            "980:7\n",
            "1000:7\n",
            "1020:7\n",
            "1040:7\n",
            "1060:7\n",
            "1080:7\n",
            "1100:7\n",
            "1120:7\n",
            "1140:7\n",
            "1160:7\n",
            "1180:7\n",
            "1200:7\n",
            "1220:7\n",
            "1240:7\n",
            "1260:7\n",
            "1280:7\n",
            "1300:7\n",
            "1320:7\n",
            "1340:7\n",
            "1360:7\n",
            "1380:7\n",
            "1400:7\n",
            "1420:7\n",
            "1440:7\n",
            "1460:7\n",
            "1480:7\n",
            "training epoch: 7, mean loss: 0.017083508893847466\n",
            "early_stop_time/early_stop_threshold: 2/40, mean loss: 0.017171651124954224\n",
            "0:8\n",
            "20:8\n",
            "40:8\n",
            "60:8\n",
            "80:8\n",
            "100:8\n",
            "120:8\n",
            "140:8\n",
            "160:8\n",
            "180:8\n",
            "200:8\n",
            "220:8\n",
            "240:8\n",
            "260:8\n",
            "280:8\n",
            "300:8\n",
            "320:8\n",
            "340:8\n",
            "360:8\n",
            "380:8\n",
            "400:8\n",
            "420:8\n",
            "440:8\n",
            "460:8\n",
            "480:8\n",
            "500:8\n",
            "520:8\n",
            "540:8\n",
            "560:8\n",
            "580:8\n",
            "600:8\n",
            "620:8\n",
            "640:8\n",
            "660:8\n",
            "680:8\n",
            "700:8\n",
            "720:8\n",
            "740:8\n",
            "760:8\n",
            "780:8\n",
            "800:8\n",
            "820:8\n",
            "840:8\n",
            "860:8\n",
            "880:8\n",
            "900:8\n",
            "920:8\n",
            "940:8\n",
            "960:8\n",
            "980:8\n",
            "1000:8\n",
            "1020:8\n",
            "1040:8\n",
            "1060:8\n",
            "1080:8\n",
            "1100:8\n",
            "1120:8\n",
            "1140:8\n",
            "1160:8\n",
            "1180:8\n",
            "1200:8\n",
            "1220:8\n",
            "1240:8\n",
            "1260:8\n",
            "1280:8\n",
            "1300:8\n",
            "1320:8\n",
            "1340:8\n",
            "1360:8\n",
            "1380:8\n",
            "1400:8\n",
            "1420:8\n",
            "1440:8\n",
            "1460:8\n",
            "1480:8\n",
            "training epoch: 8, mean loss: 0.016892878338694572\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.016682008281350136\n",
            "0:9\n",
            "20:9\n",
            "40:9\n",
            "60:9\n",
            "80:9\n",
            "100:9\n",
            "120:9\n",
            "140:9\n",
            "160:9\n",
            "180:9\n",
            "200:9\n",
            "220:9\n",
            "240:9\n",
            "260:9\n",
            "280:9\n",
            "300:9\n",
            "320:9\n",
            "340:9\n",
            "360:9\n",
            "380:9\n",
            "400:9\n",
            "420:9\n",
            "440:9\n",
            "460:9\n",
            "480:9\n",
            "500:9\n",
            "520:9\n",
            "540:9\n",
            "560:9\n",
            "580:9\n",
            "600:9\n",
            "620:9\n",
            "640:9\n",
            "660:9\n",
            "680:9\n",
            "700:9\n",
            "720:9\n",
            "740:9\n",
            "760:9\n",
            "780:9\n",
            "800:9\n",
            "820:9\n",
            "840:9\n",
            "860:9\n",
            "880:9\n",
            "900:9\n",
            "920:9\n",
            "940:9\n",
            "960:9\n",
            "980:9\n",
            "1000:9\n",
            "1020:9\n",
            "1040:9\n",
            "1060:9\n",
            "1080:9\n",
            "1100:9\n",
            "1120:9\n",
            "1140:9\n",
            "1160:9\n",
            "1180:9\n",
            "1200:9\n",
            "1220:9\n",
            "1240:9\n",
            "1260:9\n",
            "1280:9\n",
            "1300:9\n",
            "1320:9\n",
            "1340:9\n",
            "1360:9\n",
            "1380:9\n",
            "1400:9\n",
            "1420:9\n",
            "1440:9\n",
            "1460:9\n",
            "1480:9\n",
            "training epoch: 9, mean loss: 0.016591887921094894\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.016010504215955734\n",
            "0:10\n",
            "20:10\n",
            "40:10\n",
            "60:10\n",
            "80:10\n",
            "100:10\n",
            "120:10\n",
            "140:10\n",
            "160:10\n",
            "180:10\n",
            "200:10\n",
            "220:10\n",
            "240:10\n",
            "260:10\n",
            "280:10\n",
            "300:10\n",
            "320:10\n",
            "340:10\n",
            "360:10\n",
            "380:10\n",
            "400:10\n",
            "420:10\n",
            "440:10\n",
            "460:10\n",
            "480:10\n",
            "500:10\n",
            "520:10\n",
            "540:10\n",
            "560:10\n",
            "580:10\n",
            "600:10\n",
            "620:10\n",
            "640:10\n",
            "660:10\n",
            "680:10\n",
            "700:10\n",
            "720:10\n",
            "740:10\n",
            "760:10\n",
            "780:10\n",
            "800:10\n",
            "820:10\n",
            "840:10\n",
            "860:10\n",
            "880:10\n",
            "900:10\n",
            "920:10\n",
            "940:10\n",
            "960:10\n",
            "980:10\n",
            "1000:10\n",
            "1020:10\n",
            "1040:10\n",
            "1060:10\n",
            "1080:10\n",
            "1100:10\n",
            "1120:10\n",
            "1140:10\n",
            "1160:10\n",
            "1180:10\n",
            "1200:10\n",
            "1220:10\n",
            "1240:10\n",
            "1260:10\n",
            "1280:10\n",
            "1300:10\n",
            "1320:10\n",
            "1340:10\n",
            "1360:10\n",
            "1380:10\n",
            "1400:10\n",
            "1420:10\n",
            "1440:10\n",
            "1460:10\n",
            "1480:10\n",
            "training epoch: 10, mean loss: 0.016602758318185806\n",
            "early_stop_time/early_stop_threshold: 1/40, mean loss: 0.016687268391251564\n",
            "0:11\n",
            "20:11\n",
            "40:11\n",
            "60:11\n",
            "80:11\n",
            "100:11\n",
            "120:11\n",
            "140:11\n",
            "160:11\n",
            "180:11\n",
            "200:11\n",
            "220:11\n",
            "240:11\n",
            "260:11\n",
            "280:11\n",
            "300:11\n",
            "320:11\n",
            "340:11\n",
            "360:11\n",
            "380:11\n",
            "400:11\n",
            "420:11\n",
            "440:11\n",
            "460:11\n",
            "480:11\n",
            "500:11\n",
            "520:11\n",
            "540:11\n",
            "560:11\n",
            "580:11\n",
            "600:11\n",
            "620:11\n",
            "640:11\n",
            "660:11\n",
            "680:11\n",
            "700:11\n",
            "720:11\n",
            "740:11\n",
            "760:11\n",
            "780:11\n",
            "800:11\n",
            "820:11\n",
            "840:11\n",
            "860:11\n",
            "880:11\n",
            "900:11\n",
            "920:11\n",
            "940:11\n",
            "960:11\n",
            "980:11\n",
            "1000:11\n",
            "1020:11\n",
            "1040:11\n",
            "1060:11\n",
            "1080:11\n",
            "1100:11\n",
            "1120:11\n",
            "1140:11\n",
            "1160:11\n",
            "1180:11\n",
            "1200:11\n",
            "1220:11\n",
            "1240:11\n",
            "1260:11\n",
            "1280:11\n",
            "1300:11\n",
            "1320:11\n",
            "1340:11\n",
            "1360:11\n",
            "1380:11\n",
            "1400:11\n",
            "1420:11\n",
            "1440:11\n",
            "1460:11\n",
            "1480:11\n",
            "training epoch: 11, mean loss: 0.01644727773964405\n",
            "early_stop_time/early_stop_threshold: 2/40, mean loss: 0.01688452437520027\n",
            "0:12\n",
            "20:12\n",
            "40:12\n",
            "60:12\n",
            "80:12\n",
            "100:12\n",
            "120:12\n",
            "140:12\n",
            "160:12\n",
            "180:12\n",
            "200:12\n",
            "220:12\n",
            "240:12\n",
            "260:12\n",
            "280:12\n",
            "300:12\n",
            "320:12\n",
            "340:12\n",
            "360:12\n",
            "380:12\n",
            "400:12\n",
            "420:12\n",
            "440:12\n",
            "460:12\n",
            "480:12\n",
            "500:12\n",
            "520:12\n",
            "540:12\n",
            "560:12\n",
            "580:12\n",
            "600:12\n",
            "620:12\n",
            "640:12\n",
            "660:12\n",
            "680:12\n",
            "700:12\n",
            "720:12\n",
            "740:12\n",
            "760:12\n",
            "780:12\n",
            "800:12\n",
            "820:12\n",
            "840:12\n",
            "860:12\n",
            "880:12\n",
            "900:12\n",
            "920:12\n",
            "940:12\n",
            "960:12\n",
            "980:12\n",
            "1000:12\n",
            "1020:12\n",
            "1040:12\n",
            "1060:12\n",
            "1080:12\n",
            "1100:12\n",
            "1120:12\n",
            "1140:12\n",
            "1160:12\n",
            "1180:12\n",
            "1200:12\n",
            "1220:12\n",
            "1240:12\n",
            "1260:12\n",
            "1280:12\n",
            "1300:12\n",
            "1320:12\n",
            "1340:12\n",
            "1360:12\n",
            "1380:12\n",
            "1400:12\n",
            "1420:12\n",
            "1440:12\n",
            "1460:12\n",
            "1480:12\n",
            "training epoch: 12, mean loss: 0.01621733419597149\n",
            "early_stop_time/early_stop_threshold: 3/40, mean loss: 0.016404984518885612\n",
            "0:13\n",
            "20:13\n",
            "40:13\n",
            "60:13\n",
            "80:13\n",
            "100:13\n",
            "120:13\n",
            "140:13\n",
            "160:13\n",
            "180:13\n",
            "200:13\n",
            "220:13\n",
            "240:13\n",
            "260:13\n",
            "280:13\n",
            "300:13\n",
            "320:13\n",
            "340:13\n",
            "360:13\n",
            "380:13\n",
            "400:13\n",
            "420:13\n",
            "440:13\n",
            "460:13\n",
            "480:13\n",
            "500:13\n",
            "520:13\n",
            "540:13\n",
            "560:13\n",
            "580:13\n",
            "600:13\n",
            "620:13\n",
            "640:13\n",
            "660:13\n",
            "680:13\n",
            "700:13\n",
            "720:13\n",
            "740:13\n",
            "760:13\n",
            "780:13\n",
            "800:13\n",
            "820:13\n",
            "840:13\n",
            "860:13\n",
            "880:13\n",
            "900:13\n",
            "920:13\n",
            "940:13\n",
            "960:13\n",
            "980:13\n",
            "1000:13\n",
            "1020:13\n",
            "1040:13\n",
            "1060:13\n",
            "1080:13\n",
            "1100:13\n",
            "1120:13\n",
            "1140:13\n",
            "1160:13\n",
            "1180:13\n",
            "1200:13\n",
            "1220:13\n",
            "1240:13\n",
            "1260:13\n",
            "1280:13\n",
            "1300:13\n",
            "1320:13\n",
            "1340:13\n",
            "1360:13\n",
            "1380:13\n",
            "1400:13\n",
            "1420:13\n",
            "1440:13\n",
            "1460:13\n",
            "1480:13\n",
            "training epoch: 13, mean loss: 0.016527943313121796\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.015956053510308266\n",
            "0:14\n",
            "20:14\n",
            "40:14\n",
            "60:14\n",
            "80:14\n",
            "100:14\n",
            "120:14\n",
            "140:14\n",
            "160:14\n",
            "180:14\n",
            "200:14\n",
            "220:14\n",
            "240:14\n",
            "260:14\n",
            "280:14\n",
            "300:14\n",
            "320:14\n",
            "340:14\n",
            "360:14\n",
            "380:14\n",
            "400:14\n",
            "420:14\n",
            "440:14\n",
            "460:14\n",
            "480:14\n",
            "500:14\n",
            "520:14\n",
            "540:14\n",
            "560:14\n",
            "580:14\n",
            "600:14\n",
            "620:14\n",
            "640:14\n",
            "660:14\n",
            "680:14\n",
            "700:14\n",
            "720:14\n",
            "740:14\n",
            "760:14\n",
            "780:14\n",
            "800:14\n",
            "820:14\n",
            "840:14\n",
            "860:14\n",
            "880:14\n",
            "900:14\n",
            "920:14\n",
            "940:14\n",
            "960:14\n",
            "980:14\n",
            "1000:14\n",
            "1020:14\n",
            "1040:14\n",
            "1060:14\n",
            "1080:14\n",
            "1100:14\n",
            "1120:14\n",
            "1140:14\n",
            "1160:14\n",
            "1180:14\n",
            "1200:14\n",
            "1220:14\n",
            "1240:14\n",
            "1260:14\n",
            "1280:14\n",
            "1300:14\n",
            "1320:14\n",
            "1340:14\n",
            "1360:14\n",
            "1380:14\n",
            "1400:14\n",
            "1420:14\n",
            "1440:14\n",
            "1460:14\n",
            "1480:14\n",
            "training epoch: 14, mean loss: 0.016341129317879677\n",
            "early_stop_time/early_stop_threshold: 0/40, mean loss: 0.015841713175177574\n",
            "0:15\n",
            "20:15\n",
            "40:15\n",
            "60:15\n",
            "80:15\n",
            "100:15\n",
            "120:15\n",
            "140:15\n",
            "160:15\n",
            "180:15\n",
            "200:15\n",
            "220:15\n",
            "240:15\n",
            "260:15\n",
            "280:15\n",
            "300:15\n",
            "320:15\n",
            "340:15\n",
            "360:15\n",
            "380:15\n",
            "400:15\n",
            "420:15\n",
            "440:15\n",
            "460:15\n",
            "480:15\n",
            "500:15\n",
            "520:15\n",
            "540:15\n",
            "560:15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample(images, texts):\n",
        "    _, figs= plt.subplots(1, len(images), figsize= (12, 12))\n",
        "    for text, f, img in zip(texts, figs, images):\n",
        "        f.imshow(img.view(28, 28), cmap= 'gray')\n",
        "        f.axes.get_xaxis().set_visible(False)\n",
        "        f.axes.get_yaxis().set_visible(False)\n",
        "        f.text(0.5, 0, text, ha= 'center', va= 'bottom', fontsize= 12, color= 'white', backgroundcolor= 'black')\n",
        "    plt.show()\n",
        "\n",
        "xt, images, texts= torch.randn((1, 1, 28, 28), device= device), [], []\n",
        "u_net= torch.load(f'{save_dir}')\n",
        "dm= DenoiseDiffusion(u_net, 1000, device= device)\n",
        "for t in reversed(range(1000)):\n",
        "    xt_1= dm.p_sample(xt, torch.tensor([t]).to(device))\n",
        "    xt= xt_1\n",
        "    if (t+ 1)% 100== 1:\n",
        "        images.append(xt.view(1, 28, 28).to('cpu').detach())\n",
        "        texts.append(t+ 1)\n",
        "\n",
        "images_= torch.stack(images, dim= 0)\n",
        "show_sample(images_, texts)"
      ],
      "metadata": {
        "id": "OMwRqjCcyCxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}